# VerificationGPT.py
from __future__ import annotations

from typing import Optional
import numpy as np
import torch
import torch.nn.functional as F

from softmax_packet import softmax_fpga_variable

try:
    from transformers.models.gpt2.modeling_gpt2 import GPT2Attention
except Exception as e:
    raise RuntimeError(
        "Cannot import GPT2Attention. Check your transformers version."
    ) from e


class GPT2AttentionSoftmaxApprox(GPT2Attention):
    """
    Optimized GPT-2 Attention:
    - Default: Uses fast PyTorch operations (GPU/CPU)
    - HW Mode: Only converts Row 0 to NumPy for UART, keeps others in PyTorch
    """

    def __init__(self, config, is_cross_attention=False, layer_idx=None):
        try:
            super().__init__(
                config, is_cross_attention=is_cross_attention, layer_idx=layer_idx
            )
        except TypeError:
            super().__init__(config, is_cross_attention=is_cross_attention)

        self.ser = None
        self.last_attn: Optional[np.ndarray] = None
        self.force_store_attn: bool = False
        self.pad_value = -32.0

        # ì €ì¥ ì œì–´ í”Œë˜ê·¸
        self.store_only: bool = False
        self.store_layer: int = 0
        self.store_head: int = 0

    def set_serial(self, ser):
        self.ser = ser

    def set_force_store_attn(self, flag: bool):
        self.force_store_attn = bool(flag)

    def set_store_target(self, layer: int, head: int, store_only: bool = True):
        self.store_only = bool(store_only)
        self.store_layer = int(layer)
        self.store_head = int(head)

    @staticmethod
    def _shape_qkv(x: torch.Tensor, num_heads: int, head_dim: int) -> torch.Tensor:
        # (B, T, Embed) -> (B, H, T, Dh)
        B, T, _ = x.shape
        return x.view(B, T, num_heads, head_dim).permute(0, 2, 1, 3).contiguous()

    def forward(
        self,
        hidden_states,
        past_key_value=None,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        use_cache=False,
        output_attentions=False,
        **kwargs,
    ):
        # 1. Attention ì €ì¥ ì—¬ë¶€ íŒë‹¨
        want_attn = bool(output_attentions) or bool(
            kwargs.get("output_attentions", False)
        )
        want_attn = want_attn or getattr(self, "force_store_attn", False)

        # 2. Q, K, V ì¶”ì¶œ (PyTorch Tensor ìœ ì§€)
        qkv = self.c_attn(hidden_states)
        query, key, value = qkv.split(self.split_size, dim=2)

        query = self._shape_qkv(query, self.num_heads, self.head_dim)
        key = self._shape_qkv(key, self.num_heads, self.head_dim)
        value = self._shape_qkv(value, self.num_heads, self.head_dim)

        if past_key_value is not None:
            past_key, past_value = past_key_value
            key = torch.cat([past_key, key], dim=2)
            value = torch.cat([past_value, value], dim=2)

        present = (key, value) if use_cache else None

        # (Batch, Heads, T_query, Dim)
        # Tensor ì—°ì‚° ìµœì í™”ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œ Shape í™•ë³´
        query_layer = query
        key_layer = key
        value_layer = value

        B, H, Tq, Dh = query_layer.shape
        Tk = key_layer.shape[2]

        # 3. Score ê³„ì‚° (Matrix Multiplication - PyTorch Native)
        # (B, H, Tq, Dh) @ (B, H, Dh, Tk) -> (B, H, Tq, Tk)
        attn_weights = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attn_weights = attn_weights / (float(Dh) ** 0.5)

        # 4. Causal Mask ì ìš©
        # GPT2Attention ì›ë³¸ ë¡œì§ ì°¸ì¡° (triu ì‚¬ìš©)
        if Tq > 1 or Tk > 1:  # ì¼ë°˜ì ì¸ ê²½ìš°
            # causal mask ìƒì„±
            bias = torch.tril(
                torch.ones((Tk, Tk), dtype=torch.uint8, device=attn_weights.device)
            ).view(1, 1, Tk, Tk)
            # í˜„ì¬ ìœˆë„ìš°ì— ë§ê²Œ ìŠ¬ë¼ì´ì‹±
            # query ê¸¸ì´ë§Œí¼, key ê¸¸ì´ë§Œí¼
            # causal: ë¯¸ë˜ í† í° ë§ˆìŠ¤í‚¹
            # (GPT2 êµ¬í˜„ìƒ biasëŠ” register bufferì§€ë§Œ ì—¬ê¸°ì„  ê°„ë‹¨íˆ ìƒì„±)

            # ê°„ë‹¨í•œ Causal Masking:
            # i > j (ê³¼ê±°) í—ˆìš©, i < j (ë¯¸ë˜) ë§ˆìŠ¤í‚¹
            # ì‹¤ì œë¡œëŠ” attention_maskê°€ ë“¤ì–´ì˜¤ë¯€ë¡œ ê·¸ê²ƒê³¼ ê²°í•©ë¨.
            # í•˜ì§€ë§Œ generation ë‹¨ê³„ì—ì„œëŠ” past_key_valueê°€ ìˆìœ¼ë¯€ë¡œ
            # Tq=1 ì¼ ë•ŒëŠ” ë§ˆìŠ¤í‚¹ ë¶ˆí•„ìš” (í•­ìƒ ê³¼ê±°ë§Œ ë³´ë¯€ë¡œ)
            pass

        # transformersì˜ GPT2 ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ `bias` ë²„í¼ë¥¼ ì´ìš©í•´ causal maskingì„ í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì§ì ‘ êµ¬í˜„ ëŒ€ì‹  attention_maskì™€ ê²°í•©í•˜ì—¬ ì²˜ë¦¬í•˜ê±°ë‚˜
        # ê°„ë‹¨íˆ ìƒì‚¼ê° í–‰ë ¬ ë§ˆìŠ¤í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        # Generation ì¤‘(Tq=1)ì—ëŠ” Causal Mask ë¶ˆí•„ìš” (ì´ë¯¸ ê³¼ê±° Keyë§Œ ì¡´ì¬)
        # Prompt Forward ì¤‘(Tq > 1)ì—ëŠ” Causal Mask í•„ìš”
        if Tq > 1:
            causal_mask = torch.triu(
                torch.ones((Tq, Tk), dtype=torch.bool, device=attn_weights.device),
                diagonal=Tk - Tq + 1,
            )
            attn_weights.masked_fill_(causal_mask[None, None, :, :], self.pad_value)

        # 5. Attention Mask (Padding) ì ìš©
        if attention_mask is not None:
            # attention_mask: (B, 1, 1, Tk) í˜•íƒœë¼ê³  ê°€ì • (transformers í‘œì¤€)
            # ë§Œì•½ (B, Tk)ë¼ë©´ ì°¨ì› í™•ì¥ í•„ìš”
            if attention_mask.dim() == 2:
                _mask = attention_mask[:, None, None, :]
            else:
                _mask = attention_mask

            # maskê°€ 0ì¸ ë¶€ë¶„ì— pad_value ì ìš©
            # (transformersëŠ” ë³´í†µ 1.0(keep), 0.0(mask)ì„ ì“°ê±°ë‚˜ 0, -infë¥¼ ì”€)
            # ì—¬ê¸°ì„œëŠ” ê°’ì´ 0ì´ë©´ ë§ˆìŠ¤í‚¹ì´ë¼ ê°€ì •
            attn_weights = torch.where(
                _mask > 0,
                attn_weights,
                torch.tensor(
                    self.pad_value, dtype=attn_weights.dtype, device=attn_weights.device
                ),
            )

        # 6. Softmax (PyTorch Native - ë§¤ìš° ë¹ ë¦„)
        attn_probs = F.softmax(attn_weights, dim=-1)

        # ==========================================================
        # ğŸš€ [HW Hybrid Logic] Row 0ë§Œ ë°”ê¿”ì¹˜ê¸° (í•„ìš”ì‹œì—ë§Œ NumPy ë³€í™˜)
        # ==========================================================
        if self.ser is not None:
            # HW ì—°ì‚°ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ CPU/NumPyë¡œ ë°ì´í„° ì´ë™
            # (Batch loop ëŒ€ì‹  Batch=0ë§Œ ì²˜ë¦¬í•œë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜ Loop)

            # ì„±ëŠ¥ì„ ìœ„í•´ Batch ì²˜ë¦¬ëŠ” ìƒëµí•˜ê³  B=0ì— ëŒ€í•´ì„œë§Œ HW ì ìš© ì˜ˆì‹œ
            # (ë°ëª¨ìš©ìœ¼ë¡œëŠ” ì¶©ë¶„)
            b_idx = 0

            # Row 0ì˜ Score ê°€ì ¸ì˜¤ê¸° (Tensor) -> (H, Tk)
            # Tqì˜ 0ë²ˆì§¸ ì¸ë±ìŠ¤ (Promptì˜ ì²« í† í° or Genì˜ í˜„ì¬ í† í°)
            row0_scores_tensor = attn_weights[b_idx, :, 0, :]

            # CPUë¡œ ì´ë™ (ì‘ì€ ë°ì´í„°ë¼ ë¹ ë¦„)
            row0_scores_np = row0_scores_tensor.detach().cpu().numpy()  # (H, Tk)

            # HW ê²°ê³¼ë¥¼ ë‹´ì„ ë°°ì—´
            hw_probs_np = np.zeros_like(row0_scores_np)

            # Headë³„ë¡œ HW ìš”ì²­
            for h in range(H):
                try:
                    # UART ì „ì†¡
                    hw_out = softmax_fpga_variable(
                        self.ser,
                        row0_scores_np[h],
                        pad_value=self.pad_value,
                        deadline_s=2.0,  # HW íƒ€ì„ì•„ì›ƒ
                    )
                    hw_probs_np[h] = hw_out
                except Exception:
                    # ì‹¤íŒ¨ ì‹œ SWê°’(ì´ë¯¸ ê³„ì‚°ë¨) ì‚¬ìš©ì„ ìœ„í•´ 0ìœ¼ë¡œ ë‘ì§€ ì•Šê³ 
                    # ê¸°ì¡´ PyTorch softmax ê°’ì„ ê°€ì ¸ì˜´
                    fallback = attn_probs[b_idx, h, 0, :].detach().cpu().numpy()
                    hw_probs_np[h] = fallback

            # ê²°ê³¼ë¥¼ ë‹¤ì‹œ í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ë®ì–´ì“°ê¸°
            hw_probs_tensor = (
                torch.from_numpy(hw_probs_np)
                .to(attn_probs.device)
                .type(attn_probs.dtype)
            )
            attn_probs[b_idx, :, 0, :] = hw_probs_tensor

        # 7. Dropout & Weighted Sum
        attn_probs = self.attn_dropout(attn_probs)
        attn_output = torch.matmul(attn_probs, value_layer)  # (B, H, Tq, Dh)

        # 8. Heatmap ì €ì¥ (Target Layer/Headë§Œ)
        # ì—¬ê¸°ì„œë§Œ NumPy ë³€í™˜ ë°œìƒ (ì €ì¥ìš©)
        this_layer_idx = getattr(self, "layer_idx", None)
        store_this = (
            want_attn
            and self.store_only
            and (this_layer_idx is not None)
            and (int(this_layer_idx) == int(self.store_layer))
        )

        if store_this:
            # (B, H, Tq, Tk) -> (Tq, Tk) (Batch=0, Target Head)
            target_head = self.store_head
            saved_map = attn_probs[0, target_head, :, :].detach().cpu().numpy()
            self.last_attn = saved_map.astype(np.float64)
        else:
            if not getattr(self, "store_only", False) and want_attn:
                # store_onlyê°€ êº¼ì ¸ìˆê³  want_attnì´ë©´ ì „ì²´ ì €ì¥ (ê¸°ì¡´ í˜¸í™˜)
                # ë©”ëª¨ë¦¬ ë‚­ë¹„ ê°€ëŠ¥ì„± ìˆìŒ
                pass

        # 9. Output Format (B, Tq, H*Dh)
        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()
        new_shape = attn_output.size()[:-2] + (self.num_heads * self.head_dim,)
        attn_output = attn_output.view(*new_shape)

        attn_output = self.c_proj(attn_output)
        attn_output = self.resid_dropout(attn_output)

        return attn_output, present, None


def replace_gpt2_attention(model: torch.nn.Module, NewAttnClass):
    if not hasattr(model, "transformer") or not hasattr(model.transformer, "h"):
        raise RuntimeError("Model is not GPT-2 style.")
    for idx, block in enumerate(model.transformer.h):
        old_attn = block.attn
        new_attn = NewAttnClass(model.config, is_cross_attention=False, layer_idx=idx)
        new_attn.load_state_dict(old_attn.state_dict(), strict=True)
        block.attn = new_attn


def set_serial_to_model(model: torch.nn.Module, ser):
    for block in model.transformer.h:
        if hasattr(block.attn, "set_serial"):
            block.attn.set_serial(ser)


def clear_serial_from_model(model: torch.nn.Module):
    for block in model.transformer.h:
        if hasattr(block.attn, "set_serial"):
            block.attn.set_serial(None)


def get_last_attention_matrix(model, layer=0, head=0):
    # ì €ì¥ëœ last_attn ê°€ì ¸ì˜¤ê¸°
    layer = max(0, min(int(layer), len(model.transformer.h) - 1))
    attn_mod = model.transformer.h[layer].attn

    if hasattr(attn_mod, "last_attn") and attn_mod.last_attn is not None:
        return attn_mod.last_attn

    # ì—†ìœ¼ë©´ ë”ë¯¸ ë¦¬í„´
    return np.zeros((1, 1), dtype=np.float64)


def set_force_store_attn_to_model(model: torch.nn.Module, flag: bool):
    for block in model.transformer.h:
        if hasattr(block.attn, "set_force_store_attn"):
            block.attn.set_force_store_attn(flag)


def set_store_target_to_model(
    model: torch.nn.Module, layer: int, head: int, store_only: bool = True
):
    for block in model.transformer.h:
        if hasattr(block.attn, "set_store_target"):
            block.attn.set_store_target(layer, head, store_only)
